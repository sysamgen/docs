Pipelines
==========

.. jinja:: dags_ctx
    :debug:

    {% for dag in dags %}

    {{ dag }}
    --------------------------------
    .. image:: {{ dag }}.png

    Description
    ***********

    .. mdinclude:: docs/dags/{{ dag }}-desc.md

    Input parameters
    ****************

    `sample_input.json`

    .. literalinclude:: {{ dag }}-test_input_params.json

    Sample REST API call
    ********************

    .. code-block:: bash

        curl -v -g -X POST \
          http://localhost:8080/api/v1/dags/{{ dag }}/dagRuns \
          -H 'Cache-Control: no-cache' \
          -H 'Content-Type: application/json' \
          -d "{'conf':\'$(cat sample_input.json | sed 's/\"/\\"/g')\'}"

    Sample Airflow cli invocation
    *****************************

    .. code-block:: bash

        airflow dags trigger \
        -c "$(cat sample_input.json)" \
        -r dag_run-1 {{ dag }}

    {% endfor %}

Output data structure
-----------------------
All files defined as outputs in bioinformatics pipelines are stored in common place. Typically, these files include processed BAM, CRAM, VCFs and QC reports.
The structure is organized as follows. 
Top level category is analysis type (pipeline name). Under specific analysis outputs from many pipelines runs are stored. 
As each run has it's own unique name is used to create directories for saving pipeline's data.    

Sample structure ::

    |-- analysis_type                
    |   |-- pipeline_run_unique_id   
    |   |   |-- output_file_1     
    |   |   |-- output_file_2
    |   |   |-- output_file_3
    |   |   |-- output_file_n
    |   |-- pipeline_run_unique_id 
    |   |   |-- output_file_1     
    |   |   |-- output_file_2
    |   |   |-- output_file_3
    |   |   |-- output_file_n
    |-- analysis_type   
    |   |-- pipeline_run_unique_id
    |   |   |-- output_file_1     
    |   |   |-- output_file_2
    |   |   |-- output_file_3
    |   |   |-- output_file_n
    
